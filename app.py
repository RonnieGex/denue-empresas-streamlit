# app.py
import streamlit as st
import pandas as pd
import folium
import re
import unicodedata
import numpy as np
from folium.plugins import FastMarkerCluster
from streamlit_folium import st_folium
from openai import OpenAI
from io import BytesIO
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import hashlib

# Configuraci√≥n inicial
st.set_page_config(
    page_title="Business Optimizer Pro",
    layout="wide",
    initial_sidebar_state="expanded",
    page_icon="üöÄ"
)

# Constantes basadas en diccionario DENUE
REQUIRED_COLUMNS = {
    'nom_estab': ['nom_estab'],
    'nombre_act': ['nombre_act'],
    'per_ocu': ['per_ocu'],
    'telefono': ['telefono'],
    'correoelec': ['correoelec'],
    'www': ['www'],
    'municipio': ['municipio'],
    'localidad': ['localidad'],
    'entidad': ['entidad'],
    'latitud': ['latitud'],
    'longitud': ['longitud']
}

COLUMN_NAMES_MAP = {
    'nom_estab': 'Nombre_Negocio',
    'nombre_act': 'Giro_Principal',
    'per_ocu': 'Personal_Ocupado',
    'telefono': 'Telefono',
    'correoelec': 'Email',
    'www': 'Sitio_Web',
    'municipio': 'Municipio',
    'localidad': 'Localidad',
    'entidad': 'Estado',
    'latitud': 'Latitud',
    'longitud': 'Longitud'
}

# Funci√≥n de seguridad
def encrypt_data(data):
    return hashlib.sha256(data.encode()).hexdigest()

def normalize_column_name(col_name):
    """Normaliza nombres de columnas seg√∫n DENUE"""
    nfkd = unicodedata.normalize('NFKD', str(col_name))
    return ''.join([c for c in nfkd if not unicodedata.combining(c)])\
        .lower().strip().replace(' ', '_').split('[')[0]

@st.cache_data(ttl=3600, show_spinner=False)
def load_and_process(uploaded_file):
    """Carga y procesamiento optimizado"""
    progress = st.progress(0, text="Iniciando procesamiento...")
    
    try:
        # Lectura segura de archivos grandes
        progress.progress(10, "Leyendo archivo...")
        if uploaded_file.name.endswith('.csv'):
            chunks = pd.read_csv(
                uploaded_file,
                encoding='latin1',
                chunksize=50000,
                dtype={'telefono': 'string', 'correoelec': 'string'}
            )
            df = pd.concat(chunks)
        else:
            df = pd.read_excel(uploaded_file, engine='openpyxl')

        # Normalizaci√≥n y validaci√≥n
        progress.progress(30, "Estandarizando datos...")
        df.columns = [normalize_column_name(col) for col in df.columns]
        
        # Mapeo exacto de columnas
        rename_mapping = {}
        for target_col, possible_cols in REQUIRED_COLUMNS.items():
            for col in possible_cols:
                if col in df.columns:
                    rename_mapping[col] = target_col
        df = df.rename(columns=rename_mapping)
        df = df.rename(columns=COLUMN_NAMES_MAP)

        # Validaci√≥n cr√≠tica
        required = list(COLUMN_NAMES_MAP.values())
        missing = [col for col in required if col not in df.columns]
        if missing:
            st.error(f"Columnas requeridas faltantes: {', '.join(missing)}")
            st.stop()

        # Transformaci√≥n de datos
        progress.progress(50, "Procesando informaci√≥n...")
        df['Tama√±o_Empresa'] = df['Personal_Ocupado'].apply(
            lambda x: 'PYME' if x <= 5 else 'Mediana' if x <= 100 else 'Grande'
        )
        
        # Optimizaci√≥n de tipos
        dtypes = {
            'Latitud': 'float32',
            'Longitud': 'float32',
            'Municipio': 'category',
            'Estado': 'category'
        }
        df = df.astype(dtypes, errors='ignore')

        # Limpieza final
        progress.progress(80, "Depurando datos...")
        df = df.dropna(subset=['Estado', 'Municipio', 'Giro_Principal'])
        
        progress.progress(100, "¬°Proceso completado!")
        return df

    except Exception as e:
        progress.empty()
        st.error(f"Error cr√≠tico: {str(e)}")
        st.stop()

@st.cache_data(ttl=3600)
def analyze_with_ai(_df, api_key):
    """An√°lisis predictivo integrado"""
    try:
        client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com/v1")
        
        # Clusterizaci√≥n avanzada
        numeric_cols = _df.select_dtypes(include=[np.number]).columns
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(_df[numeric_cols].fillna(0))
        
        kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
        _df['Segmento_IA'] = kmeans.fit_predict(scaled_data)
        
        # Generaci√≥n de recomendaciones
        context = {
            'giros_top': _df['Giro_Principal'].value_counts().nlargest(5).index.tolist(),
            'empleados_promedio': round(_df['Personal_Ocupado'].mean()),
            'ubicaciones_clave': _df.groupby('Municipio')['Segmento_IA'].count().nlargest(3).index.tolist()
        }
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[{
                "role": "system",
                "content": "Eres un experto en marketing digital B2B. Genera recomendaciones basadas en:"
            },{
                "role": "user",
                "content": f"{context}\n\nSugiere estrategias segmentadas para Google Ads:"
            }],
            temperature=0.6,
            max_tokens=600
        )
        
        return {
            'df': _df,
            'analisis': response.choices[0].message.content,
            'sugerencias': context
        }
        
    except Exception as e:
        st.error(f"Error en an√°lisis IA: {str(e)}")
        return None

def create_interactive_map(df):
    """Visualizaci√≥n geoespacial optimizada"""
    if df.empty:
        st.warning("No hay datos para mostrar")
        return
    
    map_center = [df['Latitud'].mean(), df['Longitud'].mean()]
    with st.spinner("Generando mapa..."):
        m = folium.Map(location=map_center, zoom_start=10, tiles='cartodbpositron')
        FastMarkerCluster(data=df[['Latitud', 'Longitud']].values.tolist()).add_to(m)
        st_folium(m, width=1200, height=600)

def prepare_google_ads_data(df):
    """Transformaci√≥n para Google Ads"""
    return df[[
        'Nombre_Negocio',
        'Giro_Principal',
        'Tama√±o_Empresa',
        'Telefono',
        'Email',
        'Sitio_Web',
        'Municipio',
        'Estado',
        'Latitud',
        'Longitud'
    ]].rename(columns={
        'Nombre_Negocio': 'Business Name',
        'Giro_Principal': 'Industry Category',
        'Tama√±o_Empresa': 'Business Size',
        'Telefono': 'Phone',
        'Email': 'Contact Email',
        'Sitio_Web': 'Website',
        'Municipio': 'City',
        'Estado': 'State',
        'Latitud': 'Latitude',
        'Longitud': 'Longitude'
    }).dropna()

def main():
    # Estado de sesi√≥n
    if 'api_key' not in st.session_state:
        st.session_state.api_key = None
    if 'processed_data' not in st.session_state:
        st.session_state.processed_data = None

    # Interfaz principal
    st.title("üöÄ Optimizador para Google Ads Pro")
    st.markdown("Transformaci√≥n inteligente de datos empresariales")

    # Configuraci√≥n de API segura
    with st.expander("üîê Configuraci√≥n Avanzada", expanded=False):
        api_input = st.text_input("Clave API", type="password", help="Requerida para an√°lisis predictivo")
        if api_input:
            st.session_state.api_key = encrypt_data(api_input)
            st.success("Configuraci√≥n guardada exitosamente")

    # Carga de archivo
    uploaded_file = st.file_uploader(
        "Sube tu base de datos (CSV/Excel)",
        type=["csv", "xlsx"],
        help="Tama√±o m√°ximo recomendado: 300MB"
    )

    # Procesamiento autom√°tico
    if uploaded_file and st.session_state.api_key:
        if st.session_state.processed_data is None or uploaded_file.file_id != st.session_state.get('file_id'):
            with st.status("Procesando...", expanded=True) as status:
                try:
                    st.write("üîç Validando estructura...")
                    df = load_and_process(uploaded_file)
                    
                    st.write("üß† Analizando con IA...")
                    result = analyze_with_ai(df, st.session_state.api_key)
                    
                    if result:
                        st.session_state.processed_data = result
                        st.session_state.file_id = uploaded_file.file_id
                        status.update(label="An√°lisis completo ‚úÖ", state="complete")
                except Exception as e:
                    st.error(f"Error de procesamiento: {str(e)}")
                    st.session_state.processed_data = None

    # Resultados y exportaci√≥n
    if st.session_state.processed_data:
        st.markdown("## üìä Resultados del An√°lisis")
        
        with st.container():
            st.markdown("### üí° Recomendaciones Estrat√©gicas")
            st.write(st.session_state.processed_data['analisis'])
        
        # Filtros interactivos
        col1, col2 = st.columns(2)
        with col1:
            selected_giros = st.multiselect(
                "Seleccionar giros",
                options=st.session_state.processed_data['sugerencias']['giros_top'],
                default=st.session_state.processed_data['sugerencias']['giros_top'][:2]
            )
        with col2:
            selected_ubicaciones = st.multiselect(
                "Filtrar ubicaciones",
                options=st.session_state.processed_data['sugerencias']['ubicaciones_clave'],
                default=st.session_state.processed_data['sugerencias']['ubicaciones_clave']
            )
        
        # Aplicar filtros
        filtered_df = st.session_state.processed_data['df'][
            (st.session_state.processed_data['df']['Giro_Principal'].isin(selected_giros)) &
            (st.session_state.processed_data['df']['Municipio'].isin(selected_ubicaciones))
        ]
        
        # Visualizaci√≥n
        st.markdown("### üåç Mapa de Concentraci√≥n")
        create_interactive_map(filtered_df)
        
        # Exportaci√≥n
        st.markdown("## üì§ Exportar para Google Ads")
        export_format = st.radio("Formato:", ["CSV", "Excel"], horizontal=True)
        
        google_ads_data = prepare_google_ads_data(filtered_df)
        if export_format == "CSV":
            data = google_ads_data.to_csv(index=False).encode('utf-8')
        else:
            output = BytesIO()
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                google_ads_data.to_excel(writer, index=False)
            data = output.getvalue()
        
        st.download_button(
            "Descargar Dataset Optimizado",
            data=data,
            file_name=f"google_ads_data_{pd.Timestamp.now().strftime('%Y%m%d')}.{export_format.lower()}",
            mime='text/csv' if export_format == "CSV" else 'application/vnd.ms-excel'
        )

if __name__ == "__main__":
    main()
